# Sprint 2: Develop, Test and Deploy MLOps (MLOps level 1)

Overall, for this **MLOps Level 1** stage (Continuous training), new components are required to store the feature for training and inference, a trigger that notifies the potential model performance drops, and model/pipeline metadata management to store ML system details. We must modify existing codes in [Sprint 1](./sprint1.md) and break the project into several tasks to **ClearML** and use `PipelineController.add_step()` to execute them:

- Pipeline is a way to streamline and connect multiple processes, plugging the output of one process as the input of another.
- Pipelines can be executed locally or on any machine using the `clearml-agent`.
- We need to hold **ClearML** agent from Colab for GPU utilization.
- Later on, we will add UI/UX and CI/CD to it in [Sprint 3](./sprint3.md).

For the product MLOPs, it is required to use **ClearML** to:

- Break each component of data engineering, each component of training, and evaluation into many ClearML tasks.
- Chain the above tasks together and deploy an end-to-end ML pipeline of everything with model performance metrics (e.g., scalars, plots, confusion matrix, debug samples, etc.).
- Perform ML pipeline automation for Continuous Training.
- The whole process should be run again whenever new data comes in.

Firstly, I had to initialize ClearML and configure the agent. I used Colab as an agent for GPU. For each task, when I finished the development, I will use `start_locally()` to test it on my machine first and then use `execute_remotely()` to turn it into **DRAFT** mode on **ClearML**, making it as a "template" that's waiting to be enqueued and executed on the remote agent (Colab). As usual, I will set up **ClearML** according to its documentation. However, as our repo is private, I had to do an extra step: add GitHub's access tokens so that the agent can fully access the repo.

Before, when I learned about ClearML in class, I could successfully execute the example pipeline locally, but when I tried to execute it on an agent, it didn't work because it couldn't find the imported modules in the executed file. This is because the agent will create a completely new environment to execute only that file, so it doesn't know the related Python modules imported inside it. I knew why, but I didn't know how to fix this yet, and I thought the "agent" thing was so stupid. When adapting ClearML to this ASL project, I found the solution. **ClearML** will work best with git as it will clone the repo when initializing the new environment. Now, my code works like a charm.

Next, I started to store the **WLASL** raw data as an initial feature store for the pipeline. This task is quite simple. I watched [ClearML's Onboarding videos on YouTube](https://www.youtube.com/playlist?list=PLMdIlCuMqSTnb8sOFLujZufc_XGKOHdHR) and found that I just need to use **ClearML Data CLI** to add 2 related **WLASL** dataset zip files to **ClearML**. Then, I need to automate the Landmark Extraction into a **ClearML task**. In fact, this task should be put as a step in the pipeline. However, as demonstrated in the [last Sprint](./sprint1.md), this task takes an extremely long time to finish (2 days), so I named it [step 0](../preprocessing/wlasl2landmarks.py) and executed it individually for some iterations to demonstrate that my code works. I will use the final `WLASL_landmarks.npz` and the metadata files I obtained from [Sprint 1](./sprint1.md) as the outcome for this step. I uploaded them to ClearML using the **Data CLI** and the raw data uploaded before as the parents.

Now, it's time for Data Preparation Automation. Here, I just need to successfully execute [step 1](../pose2gloss/step1_data_splitting.py), then the other data engineering steps are pretty simple as they are mostly the same. The workflow would be landmarks data -> train/val/test data ([step 1](../pose2gloss/step1_data_splitting.py)) -> augment only the train data ([step 2](../pose2gloss/step2_data_augmentation.py)) -> perform padding/truncation and label encoding for all subsets ([step 3](../pose2gloss/step3_data_transformation.py)). For easy customization and chaining when including each step in the `PipelineController`, I will set up their own parameters and configure them so they can upload their outcomes as artifacts to **ClearML**.

Regarding step 1, I also added some reports and visualizations to demonstrate the splitting results, which can be viewed using the **PLOTS** tab of this step after executing the pipeline. Additionally, this step 1 was super slow when I tested it on my local machine, as it had to download nearly **10 GB** of data. Yes, **10 GB**; I thought it only downloaded the dataset with the ID I specified. However, it also downloaded all the previous versions (parents) of that data, which resulted in unnecessary, heavy, and duplicated files on my laptop. I think I will post an issue on **ClearML** about this non-optimal problem. I decided to use the agent (Colab) to execute this step, as Colab has a much faster internet connection, and I indeed obtained the results much faster than on my laptop.

After completing the Data Preparation Automation, I will automate the ML pipeline to train, evaluate, and deploy the **Pose-to-Gloss Model**:

- This step will prepare TF dataset with nose normalization for the train and val subsets from the data transformation, and train the model. Here, I perform real-time logging on **ClearML** for the training process using ClearML's `logger.report_scalar` as a callback during the fitting. The final model weights will be published to this **ClearML task** using **ClearML**'s `OutputModel`.
- There are many conflicts in this step. First, when I use `execute_remotely()` to create this step as a "template" task for the pipeline, ClearML resolves the current environment on my laptop, which includes tensorflow-intel, so this will not work on the Colab agent. This is not a big deal, as I can manually remove tensorflow-intel in the **PYTHON PACKAGES** section of this task's **EXECUTION** tab.
- The bigger problem happened when I tried to enqueue this task on the agent. ClearML will create a new environment inside the agent to execute this task as usual. However, this new environment has conflicts between the installed **tensorflow** and **nvidia-cudnn-cu12** packages. **TensorFlow 2.18.0** requires `nvidia-cudnn-cu12==9.3.0.75`, but ClearML installed the **9.2** version instead. It took me quite a long time to find out what happened

Next, I created a ClearML task to evaluate the **Pose-to-Gloss** model. This task will take the train/val/test subsets from the data transformation step and the model obtained from the model training task to perform the evaluations on each subset. Here, I exported 3 reports to **ClearML**: sklearn's `classification_report`, confusion matrix, and a summarized metrics table of subsets (loss, accuracy, top5 accuracy).

Now, it's time for the most important task, the continuous training pipeline for **Pose-to-Gloss**:

- I followed the **PipelineController** example I learned in Week 8 and chained each "template" task I created before to create an end-to-end ML pipeline. When executing the pipeline remotely on the agent, it didn't yield any errors, but it just got stuck there; no step in the pipeline was executed. Later, I found this GitHub issue, and I knew why: We can not use the same queue for both the pipeline and its tasks when starting the pipeline remotely, as it will make the pipeline stuck, so just use a different name for the pipeline queue or use `start_locally()`. Therefore, I decided to `start_locally()` the pipeline on my laptop, but each of its steps will be executed inside the Colab agent. I also created a pull request in the **PipelineController** example repo, documenting this bug and a way to fix it.
- Here, I executed 2 pipelines: one for **100** top glosses (**WLASL100**), one for **300** (**WLASL300**). I included each step's parameters into the pipeline's parameters so that I can configure each step in a global way through the pipeline. I just need to update some parameters of the **WLASL100** pipeline and start it again to obtain the **WLASL300** pipeline. When all the steps for each pipeline turn green to blue, I know I succeeded.

After finishing the pipeline, I decided to take an extra step to further follow the **MLOps level 1**, the **Trigger**. I developed 2 **Triggers** here to rerun the whole pipeline by monitoring these 2 events: new dataset arrival or test accuracy below 60%. An extremely useful thing I implemented here is tag management, which I learned from ClearML's tutorial. It's so useful that I decided to adapt it for the whole project, assigning each task with some tags that best describe it. By doing this, I can easily access the latest tasks or artifacts without knowing a specific ID, as ClearML will automatically check and update what I want to use via the tags. For example, in this trigger implementation, there are 2 pipelines (for **WLASL100** and **WLASL300**), but I only need to use one for production. Therefore, I decided to assign a tag production for the pipeline I want to use, which is the **WLASL100** for now. If a trigger event happens, I will rerun this production pipeline. In the future, if I want to use the pipeline for **WLASL300**, I can simply remove the production tag for the current **WLASL100** pipeline and reassign it for the **WLASL300** one. The **Trigger** will run **WLASL300** when the events happen.

I also manage the tags for the project's dataset by assigning the `‘landmarks’` tags for the processed data obtained from **step 0**. Additionally, to perform the new dataset arrival **Trigger**, I will assign the data I want to use with the tags stable and monitor if they are updated. Here, for a demo, I created a temporary dataset and assigned it 2 tags `['landmarks', 'stable']`; then I removed the stable tag and applied it to the correct data I'm using to simulate the data update to execute the **Trigger**. For the 2nd **Trigger**, I will monitor the `'Test Accuracy'` scalar from the **Model evaluation task** of the production pipeline. When I implemented the **Model evaluation task** "template", I should have exported the metrics using `logger.report_single_value`, but I forgot. Therefore, when implementing this **Trigger**, I have to write a bit more complex code to extract the test accuracy from the metrics table report of the **Model evaluation task**, but it's okay; it still works. I have to use **3 machines** to make the trigger run successfully, as the **Trigger**, the pipeline, and its steps can't be run on the same machine, or it will be stuck. Here, I use my laptop to execute the **Trigger** and create **2 Colab notebooks**, one for the pipeline execution and one for each step's execution.

Finally, I deployed the trained model as a **REST API**. I was planning to use **ClearML serving** as I really like the Triton engine and the monitoring features with **Grafana** it provided. I could run its example successfully and even perform the automatic model deployment smoothly. However, when I tried the model in this project, it kept resulting in errors, and I didn't know how to fix it. There are 5 reasons why **ClearML serving** is impossible to fix for me:

- I'm not too familiar with **Docker** things or **Triton** engines.
- Its error message is not clear: **error code -11**. I have no idea what this error is about.
- Its community is too small to have a solution for my problem. I have already searched a lot and walked through its GitHub issues, but nobody faces that same error.
- I decided to use the normal `docker-compose yaml` instead of the Triton one, but then I got a new error about GRPC. I'm also unfamiliar with GRPC, so I have no idea what this means.
- It used the `Preprocess` class to handle the model inputs and outputs. It will call `model.predict` internally, so I can't customize the model during the inference. This is quite challenging for me as my model is quite complex and requires access to its custom layers for a successful reload.

Therefore, I decided to just use **FastAPI**. It ran so smoothly and even allowed me to provide a clear data structure for the model input/output using **PyDantic**. I can develop as many endpoints as I want, such as `/health` to ensure the model is loaded or `/metadata` to retrieve metadata about the loaded model. It even created API documentation for me using its built-in **Swagger**.