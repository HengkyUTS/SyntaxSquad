{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Dependencies"
      ],
      "metadata": {
        "id": "GC81RxXBTYyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRZr0-2ORIqI",
        "outputId": "19338c1a-1fa2-4f38-a951-46b8c400cdb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to install all required dependencies\n",
        "!pip install openai torch torchvision opencv-python matplotlib pillow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "0hbPiEu0TlDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from openai import OpenAI\n",
        "import io\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Pp9Z1yLZROAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup OpenAI Client"
      ],
      "metadata": {
        "id": "SJY3zJwSToj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up OpenAI client\n",
        "# Replace with your actual API key\n",
        "openai_api_key = \"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\"  # Enter your API key here\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Test OpenAI connection\n",
        "def test_openai_connection():\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Hello, this is a test message.\"}],\n",
        "            max_tokens=10\n",
        "        )\n",
        "        print(\"OpenAI connection successful!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI connection error: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Run test\n",
        "test_openai_connection()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn5BwulkTrSG",
        "outputId": "3282bc4f-0436-4f34-c507-5d044c08fb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI connection error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define ASL Recognition Model"
      ],
      "metadata": {
        "id": "pAD2Wn27Trm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ASL recognition model\n",
        "class ASLModel(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ASLModel, self).__init__()\n",
        "        # Use a pre-trained ResNet as the base model\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "        num_features = self.base_model.fc.in_features\n",
        "        self.base_model.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "# Function to load the ASL model\n",
        "def load_asl_model():\n",
        "    model = ASLModel()\n",
        "    # In a real implementation, you would load pre-trained weights here\n",
        "    # model.load_state_dict(torch.load('wlasl100_model.pth'))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Define preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Simplified WLASL 100 class labels\n",
        "# Replace this with the actual WLASL 100 classes\n",
        "wlasl_classes = [\n",
        "    \"book\", \"want\", \"homework\", \"candy\", \"who\", \"finish\", \"go\", \"clothes\", \"shoes\", \"computer\",\n",
        "    \"drink\", \"food\", \"teacher\", \"bowling\", \"drive\", \"bed\", \"motorcycle\", \"eat\", \"baseball\", \"friend\",\n",
        "    \"fish\", \"love\", \"movie\", \"paper\", \"birthday\", \"know\", \"school\", \"study\", \"work\", \"hospital\",\n",
        "    \"more\", \"help\", \"give\", \"year\", \"hello\", \"bathroom\", \"please\", \"dog\", \"cat\", \"car\", \"blue\",\n",
        "    \"red\", \"woman\", \"man\", \"baby\", \"yes\", \"no\", \"like\", \"deaf\", \"pizza\", \"water\", \"milk\", \"music\",\n",
        "    \"time\", \"sorry\", \"good\", \"bad\", \"cold\", \"hot\", \"flower\", \"house\", \"thank you\", \"coffee\", \"now\",\n",
        "    \"before\", \"later\", \"after\", \"meet\", \"family\", \"table\", \"chair\", \"play\", \"color\", \"black\", \"white\",\n",
        "    \"green\", \"yellow\", \"orange\", \"student\", \"class\", \"write\", \"buy\", \"sell\", \"watch\", \"clock\", \"day\",\n",
        "    \"night\", \"apple\", \"orange\", \"nice\", \"people\", \"walk\", \"run\", \"sit\", \"stand\", \"call\", \"phone\",\n",
        "    \"name\", \"tomorrow\", \"yesterday\"\n",
        "]\n",
        "\n",
        "print(f\"Model defined with {len(wlasl_classes)} classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUWXAjuhTvA6",
        "outputId": "a58f3d6b-9ae4-4448-d6c7-55262a42195b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model defined with 100 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Capture Function"
      ],
      "metadata": {
        "id": "PLvghI0kTui7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved video capture function with preview display\n",
        "def capture_video_from_webcam(filename=\"video.mp4\", quality=0.8, max_seconds=10):\n",
        "    from IPython.display import display, Javascript, HTML\n",
        "    from google.colab import output\n",
        "    import base64\n",
        "    import time\n",
        "\n",
        "    # JavaScript to capture video with visible preview\n",
        "    js = Javascript('''\n",
        "    async function captureVideo(quality, maxSeconds) {\n",
        "      // Create and style container\n",
        "      const div = document.createElement('div');\n",
        "      div.style.border = '2px solid #4CAF50';\n",
        "      div.style.padding = '10px';\n",
        "      div.style.borderRadius = '5px';\n",
        "      div.style.backgroundColor = '#f8f9fa';\n",
        "      div.style.marginBottom = '20px';\n",
        "      div.style.width = 'fit-content';\n",
        "\n",
        "      // Create elements\n",
        "      const heading = document.createElement('h3');\n",
        "      heading.textContent = \"ASL Video Capture\";\n",
        "      heading.style.marginTop = '0';\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.border = '1px solid #ddd';\n",
        "      video.style.borderRadius = '3px';\n",
        "      video.width = 640;\n",
        "      video.height = 480;\n",
        "\n",
        "      const controls = document.createElement('div');\n",
        "      controls.style.marginTop = '10px';\n",
        "      controls.style.display = 'flex';\n",
        "      controls.style.gap = '10px';\n",
        "\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.backgroundColor = '#4CAF50';\n",
        "      capture.style.color = 'white';\n",
        "      capture.style.border = 'none';\n",
        "      capture.style.padding = '8px 15px';\n",
        "      capture.style.borderRadius = '4px';\n",
        "      capture.style.cursor = 'pointer';\n",
        "\n",
        "      const stopCapture = document.createElement('button');\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.backgroundColor = '#f44336';\n",
        "      stopCapture.style.color = 'white';\n",
        "      stopCapture.style.border = 'none';\n",
        "      stopCapture.style.padding = '8px 15px';\n",
        "      stopCapture.style.borderRadius = '4px';\n",
        "      stopCapture.style.cursor = 'pointer';\n",
        "      stopCapture.style.display = 'none';\n",
        "\n",
        "      const status = document.createElement('div');\n",
        "      status.style.marginTop = '10px';\n",
        "      status.textContent = \"Ready to record. Click 'Start Recording' when ready.\";\n",
        "\n",
        "      const timer = document.createElement('div');\n",
        "      timer.style.marginTop = '5px';\n",
        "      timer.style.fontWeight = 'bold';\n",
        "      timer.style.fontSize = '16px';\n",
        "\n",
        "      // Assemble the DOM\n",
        "      controls.appendChild(capture);\n",
        "      controls.appendChild(stopCapture);\n",
        "      div.appendChild(heading);\n",
        "      div.appendChild(video);\n",
        "      div.appendChild(controls);\n",
        "      div.appendChild(status);\n",
        "      div.appendChild(timer);\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      // Access the webcam\n",
        "      try {\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false});\n",
        "        window.stream = stream; // Make stream available to console for debugging\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "        status.textContent = \"Camera connected. Ready to record.\";\n",
        "      } catch (err) {\n",
        "        status.textContent = \"Error accessing camera: \" + err.message;\n",
        "        status.style.color = 'red';\n",
        "        return;\n",
        "      }\n",
        "\n",
        "      // Set up recording functionality\n",
        "      const canvas = document.createElement('canvas');\n",
        "      const ctx = canvas.getContext('2d');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "\n",
        "      const recordedChunks = [];\n",
        "      let mediaRecorder = null;\n",
        "      let startTime = null;\n",
        "      let timerInterval = null;\n",
        "\n",
        "      // Update the timer display\n",
        "      function updateTimer() {\n",
        "        if (!startTime) return;\n",
        "        const elapsed = (Date.now() - startTime) / 1000;\n",
        "        const remaining = Math.max(0, maxSeconds - elapsed);\n",
        "        timer.textContent = `Recording: ${remaining.toFixed(1)}s remaining`;\n",
        "        if (remaining <= 3) {\n",
        "          timer.style.color = 'red';\n",
        "        }\n",
        "      }\n",
        "\n",
        "      // Start recording handler\n",
        "      capture.onclick = () => {\n",
        "        try {\n",
        "          const options = {mimeType: 'video/webm; codecs=vp9'};\n",
        "          mediaRecorder = new MediaRecorder(stream, options);\n",
        "\n",
        "          mediaRecorder.ondataavailable = (e) => {\n",
        "            if (e.data.size > 0) {\n",
        "              recordedChunks.push(e.data);\n",
        "            }\n",
        "          };\n",
        "\n",
        "          mediaRecorder.start();\n",
        "          startTime = Date.now();\n",
        "          timerInterval = setInterval(updateTimer, 100);\n",
        "\n",
        "          status.textContent = \"Recording in progress...\";\n",
        "          capture.style.display = 'none';\n",
        "          stopCapture.style.display = 'block';\n",
        "\n",
        "          // Auto-stop after maxSeconds\n",
        "          setTimeout(() => {\n",
        "            if (mediaRecorder && mediaRecorder.state !== 'inactive') {\n",
        "              stopCapture.click();\n",
        "            }\n",
        "          }, maxSeconds * 1000);\n",
        "        } catch (err) {\n",
        "          status.textContent = \"Error starting recording: \" + err.message;\n",
        "          status.style.color = 'red';\n",
        "        }\n",
        "      };\n",
        "\n",
        "      // Stop recording handler\n",
        "      stopCapture.onclick = () => {\n",
        "        try {\n",
        "          if (mediaRecorder) {\n",
        "            mediaRecorder.stop();\n",
        "            clearInterval(timerInterval);\n",
        "            timer.textContent = \"Recording complete!\";\n",
        "            status.textContent = \"Processing video...\";\n",
        "\n",
        "            // Clean up\n",
        "            stopCapture.style.display = 'none';\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "\n",
        "            // Create video blob and send it back\n",
        "            setTimeout(() => {\n",
        "              const blob = new Blob(recordedChunks, {type: 'video/mp4'});\n",
        "              const reader = new FileReader();\n",
        "              reader.readAsDataURL(blob);\n",
        "              reader.onloadend = () => {\n",
        "                const base64data = reader.result;\n",
        "                status.textContent = \"Video captured successfully!\";\n",
        "                google.colab.kernel.invokeFunction('onVideoData', [base64data], {});\n",
        "              };\n",
        "            }, 1000);\n",
        "          }\n",
        "        } catch (err) {\n",
        "          status.textContent = \"Error stopping recording: \" + err.message;\n",
        "          status.style.color = 'red';\n",
        "        }\n",
        "      };\n",
        "    }\n",
        "\n",
        "    captureVideo(''' + str(quality) + ''', ''' + str(max_seconds) + ''');\n",
        "    ''')\n",
        "\n",
        "    # Display the JavaScript\n",
        "    display(js)\n",
        "\n",
        "    # Variable to store the video data\n",
        "    result = None\n",
        "\n",
        "    # Callback function to receive video data from JavaScript\n",
        "    def on_video_data(data):\n",
        "        nonlocal result\n",
        "        result = data\n",
        "\n",
        "    # Register the callback\n",
        "    output.register_callback('onVideoData', on_video_data)\n",
        "\n",
        "    # Wait for the video data\n",
        "    print(\"Waiting for video capture... (Click 'Start Recording' button when ready)\")\n",
        "    while result is None:\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    # Process the received base64 data\n",
        "    try:\n",
        "        # Parse the base64 string\n",
        "        b64_data = result.split(',')[1]\n",
        "\n",
        "        # Save the video file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(base64.b64decode(b64_data))\n",
        "\n",
        "        print(f\"Video saved to {filename}\")\n",
        "\n",
        "        # Display the saved video\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"border:2px solid #ddd; padding:10px; border-radius:5px; margin:10px 0;\">\n",
        "            <h3>Captured Video:</h3>\n",
        "            <video width=\"640\" height=\"480\" controls>\n",
        "                <source src=\"data:video/mp4;base64,{b64_data}\" type=\"video/mp4\">\n",
        "                Your browser does not support the video tag.\n",
        "            </video>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "sP2Kz1zZT08E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Processing Function"
      ],
      "metadata": {
        "id": "_mgSVro7T1RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process video for ASL recognition\n",
        "def process_video(video_path, model):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return [], []\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    print(f\"Video info: {frame_count} frames, {fps} FPS\")\n",
        "\n",
        "    # We'll sample frames at regular intervals\n",
        "    frames_to_sample = min(30, frame_count)  # Sample up to 30 frames\n",
        "    sample_interval = max(1, frame_count // frames_to_sample)\n",
        "\n",
        "    predictions = []\n",
        "    sampled_frames = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, frame_count, sample_interval):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert OpenCV BGR to RGB\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "            # Apply transformations and predict\n",
        "            input_tensor = transform(pil_image).unsqueeze(0)\n",
        "            output = model(input_tensor)\n",
        "\n",
        "            # Get top prediction\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            predictions.append(predicted.item())\n",
        "            sampled_frames.append(frame_rgb)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Determine the most common prediction (voting)\n",
        "    prediction_counts = Counter(predictions)\n",
        "    most_common_predictions = prediction_counts.most_common(3)\n",
        "\n",
        "    # Return top 3 predictions and their counts\n",
        "    top_predictions = [(wlasl_classes[pred], count) for pred, count in most_common_predictions if pred < len(wlasl_classes)]\n",
        "\n",
        "    print(f\"Top predictions: {top_predictions}\")\n",
        "\n",
        "    # Also return some sample frames for visualization\n",
        "    return top_predictions, sampled_frames\n",
        "\n",
        "# Function to display frames\n",
        "def display_frames(frames, num_frames=5):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, frame in enumerate(frames[:num_frames]):\n",
        "        plt.subplot(1, num_frames, i+1)\n",
        "        plt.imshow(frame)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"Sample frames from video\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U5dNEPJzT4CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ChatGPT Translation Function"
      ],
      "metadata": {
        "id": "jDbppMI-T5Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to send ASL predictions to ChatGPT for translation\n",
        "def get_chatgpt_translation(predictions):\n",
        "    try:\n",
        "        # Create a prompt for ChatGPT\n",
        "        signs = ', '.join([pred for pred, _ in predictions])\n",
        "        prompt = f\"I detected the following ASL signs: {signs}. Can you translate this into a meaningful sentence or phrase in English?\"\n",
        "\n",
        "        print(f\"Sending to ChatGPT: {prompt}\")\n",
        "\n",
        "        # Call ChatGPT API\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",  # You can use gpt-4 if available\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert in American Sign Language translation. Your task is to translate detected ASL signs into natural, coherent English sentences.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract and return the translation\n",
        "        translation = response.choices[0].message.content\n",
        "        return translation\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Translation error: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return error_msg"
      ],
      "metadata": {
        "id": "IVbzG2YsT8qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main translation Function"
      ],
      "metadata": {
        "id": "XeQzAM6UT9OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to orchestrate the ASL translation process\n",
        "def translate_asl():\n",
        "    print(\"Initializing ASL translation system...\")\n",
        "\n",
        "    # Load ASL recognition model\n",
        "    print(\"Loading ASL recognition model...\")\n",
        "    model = load_asl_model()\n",
        "\n",
        "    # Capture video\n",
        "    print(\"Please perform an ASL sign when ready.\")\n",
        "    print(\"Starting video capture (max 10 seconds)...\")\n",
        "    video_path = capture_video_from_webcam(max_seconds=10)\n",
        "\n",
        "    # Process video for ASL recognition\n",
        "    print(\"Processing video for ASL recognition...\")\n",
        "    predictions, frames = process_video(video_path, model)\n",
        "\n",
        "    if not predictions:\n",
        "        print(\"No ASL signs detected. Please try again.\")\n",
        "        return None\n",
        "\n",
        "    # Display some sample frames\n",
        "    display_frames(frames, 5)\n",
        "\n",
        "    # Get translation from ChatGPT\n",
        "    print(\"Getting translation from ChatGPT...\")\n",
        "    translation = get_chatgpt_translation(predictions)\n",
        "\n",
        "    print(\"\\n===== TRANSLATION RESULT =====\")\n",
        "    print(f\"Detected ASL signs: {', '.join([pred for pred, _ in predictions])}\")\n",
        "    print(f\"Translation: {translation}\")\n",
        "    print(\"==============================\\n\")\n",
        "\n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'translation': translation,\n",
        "        'video_path': video_path\n",
        "    }"
      ],
      "metadata": {
        "id": "VL94mX8uUAid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Additional Testing Function"
      ],
      "metadata": {
        "id": "JKQvxVtWUBEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Video Capture"
      ],
      "metadata": {
        "id": "j4d5kSMtV7w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_webcam_capture():\n",
        "    \"\"\"\n",
        "    Standalone function to test webcam capture in Google Colab\n",
        "    \"\"\"\n",
        "    print(\"Testing webcam capture...\")\n",
        "    video_path = capture_video_from_webcam(max_seconds=5)\n",
        "\n",
        "    if video_path:\n",
        "        print(\"✅ Video capture successful!\")\n",
        "\n",
        "        # Check if file exists and get size\n",
        "        import os\n",
        "        if os.path.exists(video_path):\n",
        "            size_kb = os.path.getsize(video_path) / 1024\n",
        "            print(f\"Video file size: {size_kb:.2f} KB\")\n",
        "\n",
        "            # Try to open the video file to confirm it's valid\n",
        "            import cv2\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if cap.isOpened():\n",
        "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "                print(f\"Video is valid: {frame_count} frames, {fps} FPS\")\n",
        "\n",
        "                # Read and display the first frame\n",
        "                ret, frame = cap.read()\n",
        "                if ret:\n",
        "                    # Convert from BGR to RGB\n",
        "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                    # Display the frame\n",
        "                    import matplotlib.pyplot as plt\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    plt.imshow(frame_rgb)\n",
        "                    plt.title(\"First Frame from Captured Video\")\n",
        "                    plt.axis('off')\n",
        "                    plt.show()\n",
        "\n",
        "                cap.release()\n",
        "            else:\n",
        "                print(\"❌ Error: Could not open video file\")\n",
        "        else:\n",
        "            print(f\"❌ Error: File {video_path} not found\")\n",
        "    else:\n",
        "        print(\"❌ Video capture failed\")\n",
        "\n",
        "# Run the test\n",
        "test_webcam_capture()"
      ],
      "metadata": {
        "id": "dJbR9HKhV_fW",
        "outputId": "bb4d9394-ef23-4d13-ab6a-b097eaee049e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing webcam capture...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function captureVideo(quality, maxSeconds) {\n",
              "      // Create and style container\n",
              "      const div = document.createElement('div');\n",
              "      div.style.border = '2px solid #4CAF50';\n",
              "      div.style.padding = '10px';\n",
              "      div.style.borderRadius = '5px';\n",
              "      div.style.backgroundColor = '#f8f9fa';\n",
              "      div.style.marginBottom = '20px';\n",
              "      div.style.width = 'fit-content';\n",
              "      \n",
              "      // Create elements\n",
              "      const heading = document.createElement('h3');\n",
              "      heading.textContent = \"ASL Video Capture\";\n",
              "      heading.style.marginTop = '0';\n",
              "      \n",
              "      const video = document.createElement('video');\n",
              "      video.style.border = '1px solid #ddd';\n",
              "      video.style.borderRadius = '3px';\n",
              "      video.width = 640;\n",
              "      video.height = 480;\n",
              "      \n",
              "      const controls = document.createElement('div');\n",
              "      controls.style.marginTop = '10px';\n",
              "      controls.style.display = 'flex';\n",
              "      controls.style.gap = '10px';\n",
              "      \n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = \"Start Recording\";\n",
              "      capture.style.backgroundColor = '#4CAF50';\n",
              "      capture.style.color = 'white';\n",
              "      capture.style.border = 'none';\n",
              "      capture.style.padding = '8px 15px';\n",
              "      capture.style.borderRadius = '4px';\n",
              "      capture.style.cursor = 'pointer';\n",
              "      \n",
              "      const stopCapture = document.createElement('button');\n",
              "      stopCapture.textContent = \"Stop Recording\";\n",
              "      stopCapture.style.backgroundColor = '#f44336';\n",
              "      stopCapture.style.color = 'white';\n",
              "      stopCapture.style.border = 'none';\n",
              "      stopCapture.style.padding = '8px 15px';\n",
              "      stopCapture.style.borderRadius = '4px';\n",
              "      stopCapture.style.cursor = 'pointer';\n",
              "      stopCapture.style.display = 'none';\n",
              "      \n",
              "      const status = document.createElement('div');\n",
              "      status.style.marginTop = '10px';\n",
              "      status.textContent = \"Ready to record. Click 'Start Recording' when ready.\";\n",
              "      \n",
              "      const timer = document.createElement('div');\n",
              "      timer.style.marginTop = '5px';\n",
              "      timer.style.fontWeight = 'bold';\n",
              "      timer.style.fontSize = '16px';\n",
              "      \n",
              "      // Assemble the DOM\n",
              "      controls.appendChild(capture);\n",
              "      controls.appendChild(stopCapture);\n",
              "      div.appendChild(heading);\n",
              "      div.appendChild(video);\n",
              "      div.appendChild(controls);\n",
              "      div.appendChild(status);\n",
              "      div.appendChild(timer);\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      // Access the webcam\n",
              "      try {\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false});\n",
              "        window.stream = stream; // Make stream available to console for debugging\n",
              "        video.srcObject = stream;\n",
              "        await video.play();\n",
              "        status.textContent = \"Camera connected. Ready to record.\";\n",
              "      } catch (err) {\n",
              "        status.textContent = \"Error accessing camera: \" + err.message;\n",
              "        status.style.color = 'red';\n",
              "        return;\n",
              "      }\n",
              "      \n",
              "      // Set up recording functionality\n",
              "      const canvas = document.createElement('canvas');\n",
              "      const ctx = canvas.getContext('2d');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      \n",
              "      const recordedChunks = [];\n",
              "      let mediaRecorder = null;\n",
              "      let startTime = null;\n",
              "      let timerInterval = null;\n",
              "      \n",
              "      // Update the timer display\n",
              "      function updateTimer() {\n",
              "        if (!startTime) return;\n",
              "        const elapsed = (Date.now() - startTime) / 1000;\n",
              "        const remaining = Math.max(0, maxSeconds - elapsed);\n",
              "        timer.textContent = `Recording: ${remaining.toFixed(1)}s remaining`;\n",
              "        if (remaining <= 3) {\n",
              "          timer.style.color = 'red';\n",
              "        }\n",
              "      }\n",
              "      \n",
              "      // Start recording handler\n",
              "      capture.onclick = () => {\n",
              "        try {\n",
              "          const options = {mimeType: 'video/webm; codecs=vp9'};\n",
              "          mediaRecorder = new MediaRecorder(stream, options);\n",
              "          \n",
              "          mediaRecorder.ondataavailable = (e) => {\n",
              "            if (e.data.size > 0) {\n",
              "              recordedChunks.push(e.data);\n",
              "            }\n",
              "          };\n",
              "          \n",
              "          mediaRecorder.start();\n",
              "          startTime = Date.now();\n",
              "          timerInterval = setInterval(updateTimer, 100);\n",
              "          \n",
              "          status.textContent = \"Recording in progress...\";\n",
              "          capture.style.display = 'none';\n",
              "          stopCapture.style.display = 'block';\n",
              "          \n",
              "          // Auto-stop after maxSeconds\n",
              "          setTimeout(() => {\n",
              "            if (mediaRecorder && mediaRecorder.state !== 'inactive') {\n",
              "              stopCapture.click();\n",
              "            }\n",
              "          }, maxSeconds * 1000);\n",
              "        } catch (err) {\n",
              "          status.textContent = \"Error starting recording: \" + err.message;\n",
              "          status.style.color = 'red';\n",
              "        }\n",
              "      };\n",
              "      \n",
              "      // Stop recording handler\n",
              "      stopCapture.onclick = () => {\n",
              "        try {\n",
              "          if (mediaRecorder) {\n",
              "            mediaRecorder.stop();\n",
              "            clearInterval(timerInterval);\n",
              "            timer.textContent = \"Recording complete!\";\n",
              "            status.textContent = \"Processing video...\";\n",
              "            \n",
              "            // Clean up\n",
              "            stopCapture.style.display = 'none';\n",
              "            stream.getTracks().forEach(track => track.stop());\n",
              "            \n",
              "            // Create video blob and send it back\n",
              "            setTimeout(() => {\n",
              "              const blob = new Blob(recordedChunks, {type: 'video/mp4'});\n",
              "              const reader = new FileReader();\n",
              "              reader.readAsDataURL(blob);\n",
              "              reader.onloadend = () => {\n",
              "                const base64data = reader.result;\n",
              "                status.textContent = \"Video captured successfully!\";\n",
              "                google.colab.kernel.invokeFunction('onVideoData', [base64data], {});\n",
              "              };\n",
              "            }, 1000);\n",
              "          }\n",
              "        } catch (err) {\n",
              "          status.textContent = \"Error stopping recording: \" + err.message;\n",
              "          status.style.color = 'red';\n",
              "        }\n",
              "      };\n",
              "    }\n",
              "    \n",
              "    captureVideo(0.8, 5);\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for video capture... (Click 'Start Recording' button when ready)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test functions for individual components\n",
        "\n",
        "def test_video_capture():\n",
        "    \"\"\"Test only the video capture component\"\"\"\n",
        "    print(\"Testing video capture...\")\n",
        "    video_path = capture_video_from_webcam(max_seconds=5)\n",
        "    print(f\"Video saved to: {video_path}\")\n",
        "    return video_path\n",
        "\n",
        "def test_video_processing(video_path=None):\n",
        "    \"\"\"Test video processing with a specified video file\"\"\"\n",
        "    if video_path is None:\n",
        "        video_path = \"video.mp4\"  # Default video path\n",
        "        print(f\"No video specified, using default: {video_path}\")\n",
        "\n",
        "    model = load_asl_model()\n",
        "    print(f\"Processing video: {video_path}\")\n",
        "    predictions, frames = process_video(video_path, model)\n",
        "    display_frames(frames)\n",
        "    return predictions, frames\n",
        "\n",
        "def test_chatgpt_translation():\n",
        "    \"\"\"Test ChatGPT translation with mock predictions\"\"\"\n",
        "    mock_predictions = [(\"hello\", 5), (\"thank you\", 3), (\"please\", 2)]\n",
        "    print(\"Testing ChatGPT translation with mock predictions...\")\n",
        "    translation = get_chatgpt_translation(mock_predictions)\n",
        "    print(f\"Translation result: {translation}\")\n",
        "    return translation\n",
        "\n",
        "# Uncomment to test individual components\n",
        "    test_video_capture()\n",
        "    test_video_processing(\"your_video_file.mp4\")\n",
        "# test_chatgpt_translation()"
      ],
      "metadata": {
        "id": "zq1wu1PeUhcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}